# Demystifying AI: Five Techniques to Explain Model Decisions
## Overview

This project reviews five key Explainable AI (XAI) techniques aimed at improving AI model transparency in critical fields like healthcare, finance, and law. The study highlights recent advancements in feature interaction modeling, local explanations, rationale extraction, biomarker discovery, and counterfactual reasoning.

## Key Techniques Covered

    Improved Shapley Value Approximations – Enhancing Kernel SHAP for dependent features.
    G-LIME (Global-Enhanced LIME) – Stabilizing local explanations using global priors.
    Selector-Predictor-Guider Framework – Extracting rationales via adversarial calibration.
    GCNN + LRP for Feature Selection – Improving stability in cancer biomarker discovery.
    Counterfactual Explanations – Bridging gaps between human and machine reasoning.

## Findings & Challenges

    Advances in stability, interpretability, and feature selection improve AI trustworthiness.
    High computational cost limits large-scale deployment.
    Lack of standardized evaluation metrics and fairness considerations.

## Future Directions

    Optimizing efficiency for large datasets.
    Establishing universal benchmarks for XAI evaluation.
    Addressing fairness and bias in AI explanations.

## Authors

    Alambuya Chelsea Peace
    Naomi Chapman
    Supervisor: Amilcar Soares
    Semester: HT24 | Course: Software Technology (2DV505)
